# Twitter Data Pipeline Project

## Overview
This goal of this project was to securely ingest, streamline and perform analysis on raw data from Twitter using the Twitter API and further transform the data using an ETL process.

## Goals
<li>Data Ingestion</li>
<li>ETL</li>
<li>Data Lake</li>
<li>Scalability</li>
<li>Cloud Processing</li>

## Tools used
<li>Amazon S3 - Data lake/Object storage</li>
<li>Amazon EC2  - Cloud computing service to process our code so we arent processing it locally.</li>
<li>Apache Airflow - Workflow Orchestration.</li>
<li>Python/Pandas</li>

## Simple Architectual Diagram
![image](https://github.com/claydoers/de-twitter-analysis-project/assets/109707159/1f721c81-d340-4cef-abe0-55700c172bcb)

## Data Snippet from JSON to CSV
<img width="1200" alt="image" src="https://github.com/claydoers/de-twitter-analysis-project/assets/109707159/0c37e4df-0e1d-4112-adb5-e9948ebb4425">

## Airflow DAG
<img width="530" alt="image" src="https://github.com/claydoers/de-twitter-analysis-project/assets/109707159/f72de77f-a4ac-4de7-82d1-9c2f34cba812">

## Simple ETL Process/function used
![image](https://github.com/claydoers/de-twitter-analysis-project/assets/109707159/faae2cd8-f53d-4bfc-b8ff-a89d5e120f5d)





